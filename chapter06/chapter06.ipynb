{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Chapter06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "英語のテキスト（nlp.txt）に対して，以下の処理を実行せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2017-05-16 03:56:39--  http://www.cl.ecei.tohoku.ac.jp/nlp100/data/nlp.txt\n",
      "Resolving www.cl.ecei.tohoku.ac.jp... 130.34.192.83\n",
      "Connecting to www.cl.ecei.tohoku.ac.jp|130.34.192.83|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8594 (8.4K) [text/plain]\n",
      "Saving to: ‘data/nlp.txt’\n",
      "\n",
      "nlp.txt             100%[===================>]   8.39K  --.-KB/s    in 0s      \n",
      "\n",
      "2017-05-16 03:56:39 (31.9 MB/s) - ‘data/nlp.txt’ saved [8594/8594]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -nc http://www.cl.ecei.tohoku.ac.jp/nlp100/data/nlp.txt -P data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing\r\n",
      "From Wikipedia, the free encyclopedia\r\n",
      "\r\n",
      "Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages. As such, NLP is related to the area of humani-computer interaction. Many challenges in NLP involve natural language understanding, that is, enabling computers to derive meaning from human or natural language input, and others involve natural language generation.\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 4 data/nlp.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 50. 文区切り\n",
    "(. or ; or : or ? or !) → 空白文字 → 英大文字というパターンを文の区切りと見なし，入力された文書を1行1文の形式で出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting q50.sh\n"
     ]
    }
   ],
   "source": [
    "%%file q50.sh\n",
    "pcregrep -o \".+?[.;:?!]\\s(?=[A-Z])\" data/nlp.txt | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages. \r\n",
      "As such, NLP is related to the area of humani-computer interaction. \r\n",
      "The history of NLP generally starts in the 1950s, although work can be found from earlier periods. \r\n",
      "The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. \r\n",
      "The authors claimed that within three or five years, machine translation would be a solved problem. \r\n",
      "However, real progress was much slower, and after the ALPAC report in 1966, which found that ten year long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. \r\n",
      "Some notably successful NLP systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 to 1966. \r\n",
      "Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. \r\n",
      "During the 1970s many programmers began to write 'conceptual ontologies', which structured real-world information into computer-understandable data. \r\n",
      "Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). \r\n"
     ]
    }
   ],
   "source": [
    "!chmod u+x q50.sh\n",
    "!./q50.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 51. 単語の切り出し\n",
    "空白を単語の区切りとみなし，50の出力を入力として受け取り，1行1単語の形式で出力せよ．ただし，文の終端では空行を出力せよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing q51.sh\n"
     ]
    }
   ],
   "source": [
    "%%file q51.sh\n",
    "pcregrep -o \".+?[.;:?!]\\s(?=[A-Z])\" data/nlp.txt |\n",
    "pcregrep -o \"[a-zA-Z]+?[\\s\\.]\" | tr \".\" \"\\n\" | tee work/nlp_parsed.txt | head -n30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural \r\n",
      "language \r\n",
      "processing \r\n",
      "is \r\n",
      "a \r\n",
      "field \r\n",
      "of \r\n",
      "computer \r\n",
      "artificial \r\n",
      "and \r\n",
      "linguistics \r\n",
      "concerned \r\n",
      "with \r\n",
      "the \r\n",
      "interactions \r\n",
      "between \r\n",
      "computers \r\n",
      "and \r\n",
      "human \r\n",
      "languages\r\n",
      "\r\n",
      "As \r\n",
      "NLP \r\n",
      "is \r\n",
      "related \r\n",
      "to \r\n",
      "the \r\n",
      "area \r\n",
      "of \r\n",
      "computer \r\n"
     ]
    }
   ],
   "source": [
    "!chmod u+x q51.sh\n",
    "!./q51.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 52. ステミング\n",
    "51の出力を入力として受け取り，Porterのステミングアルゴリズムを適用し，単語と語幹をタブ区切り形式で出力せよ． Pythonでは，Porterのステミングアルゴリズムの実装としてstemmingモジュールを利用するとよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.2.4.tar.gz (1.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.2MB 856kB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/kuribayashi/.pyenv/versions/3.5.2/lib/python3.5/site-packages (from nltk)\n",
      "Installing collected packages: nltk\n",
      "  Running setup.py install for nltk ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\n",
      "\u001b[?25hSuccessfully installed nltk-3.2.4\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing q52.py\n"
     ]
    }
   ],
   "source": [
    "%%file q52.py\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "with open(\"work/nlp_parsed.txt\", \"r\") as f1, open(\"work/nlp_stemmed.txt\", \"w\") as f2:\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed = [stemmer.stem(word.strip()) for word in f1]\n",
    "    f2.write(\"\\n\".join(stemmed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natur\r\n",
      "languag\r\n",
      "process\r\n",
      "is\r\n",
      "a\r\n",
      "field\r\n",
      "of\r\n",
      "comput\r\n",
      "artifici\r\n",
      "and\r\n"
     ]
    }
   ],
   "source": [
    "!head work/nlp_stemmed.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 53. Tokenization\n",
    "Stanford Core NLPを用い，入力テキストの解析結果をXML形式で得よ．また，このXMLファイルを読み込み，入力テキストを1行1単語の形式で出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator pos\n",
      "[main] INFO edu.stanford.nlp.tagger.maxent.MaxentTagger - Loading POS tagger from edu/stanford/nlp/models/pos-tagger/english-left3words/english-left3words-distsim.tagger ... done [1.7 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [4.3 sec].\n",
      "[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.8 sec].\n",
      "[main] INFO edu.stanford.nlp.ie.AbstractSequenceClassifier - Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.9 sec].\n",
      "[main] INFO edu.stanford.nlp.time.JollyDayHolidays - Initializing JollyDayHoliday for SUTime from classpath edu/stanford/nlp/models/sutime/jollyday/Holidays_sutime.xml as sutime.binder.1.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.5 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator dcoref\n",
      "\n",
      "Processing file /Users/kuribayashi/Documents/100knock-private/chapter06/data/nlp.txt ... writing to /Users/kuribayashi/Documents/100knock-private/chapter06/work/nlp.txt.xml\n",
      "Annotating file /Users/kuribayashi/Documents/100knock-private/chapter06/data/nlp.txt ... done [23.4 sec].\n",
      "\n",
      "Annotation pipeline timing information:\n",
      "TokenizerAnnotator: 0.1 sec.\n",
      "WordsToSentencesAnnotator: 0.0 sec.\n",
      "POSTaggerAnnotator: 0.3 sec.\n",
      "MorphaAnnotator: 0.2 sec.\n",
      "NERCombinerAnnotator: 7.0 sec.\n",
      "ParserAnnotator: 12.1 sec.\n",
      "DeterministicCorefAnnotator: 3.8 sec.\n",
      "TOTAL: 23.4 sec. for 1452 tokens at 62.0 tokens/sec.\n",
      "Pipeline setup: 15.2 sec.\n",
      "Total time for StanfordCoreNLP pipeline: 39.2 sec.\n"
     ]
    }
   ],
   "source": [
    "! java -cp \"stanford-corenlp-full-2017-06-09/*\" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators tokenize,ssplit,pos,lemma,ner,parse,dcoref -file data/nlp.txt -outputDirectory ./work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r",
      "\r\n",
      "<?xml-stylesheet href=\"CoreNLP-to-HTML.xsl\" type=\"text/xsl\"?>\r",
      "\r\n",
      "<root>\r",
      "\r\n",
      "  <document>\r",
      "\r\n",
      "    <sentences>\r",
      "\r\n",
      "      <sentence id=\"1\">\r",
      "\r\n",
      "        <tokens>\r",
      "\r\n",
      "          <token id=\"1\">\r",
      "\r\n",
      "            <word>Natural</word>\r",
      "\r\n",
      "            <lemma>natural</lemma>\r",
      "\r\n",
      "            <CharacterOffsetBegin>0</CharacterOffsetBegin>\r",
      "\r\n",
      "            <CharacterOffsetEnd>7</CharacterOffsetEnd>\r",
      "\r\n",
      "            <POS>JJ</POS>\r",
      "\r\n",
      "            <NER>O</NER>\r",
      "\r\n",
      "            <Speaker>PER0</Speaker>\r",
      "\r\n",
      "          </token>\r",
      "\r\n",
      "          <token id=\"2\">\r",
      "\r\n",
      "            <word>language</word>\r",
      "\r\n",
      "            <lemma>language</lemma>\r",
      "\r\n",
      "            <CharacterOffsetBegin>8</CharacterOffsetBegin>\r",
      "\r\n",
      "            <CharacterOffsetEnd>16</CharacterOffsetEnd>\r",
      "\r\n",
      "            <POS>NN</POS>\r",
      "\r\n",
      "            <NER>O</NER>\r",
      "\r\n",
      "            <Speaker>PER0</Speaker>\r",
      "\r\n",
      "          </token>\r",
      "\r\n",
      "          <token id=\"3\">\r",
      "\r\n",
      "            <word>processing</word>\r",
      "\r\n",
      "            <lemma>processing</lemma>\r",
      "\r\n",
      "            <CharacterOffsetBegin>17</CharacterOffsetBegin>\r",
      "\r\n",
      "            <CharacterOffsetEnd>27</CharacterOffsetEnd>\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!head -n30 work/nlp.txt.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing q53.py\n"
     ]
    }
   ],
   "source": [
    "%%file q53.py\n",
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse('work/nlp.txt.xml')\n",
    "root = tree.getroot()\n",
    "for word in root.iter('word'):\n",
    "    print(word.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural\r\n",
      "language\r\n",
      "processing\r\n",
      "From\r\n",
      "Wikipedia\r\n",
      ",\r\n",
      "the\r\n",
      "free\r\n",
      "encyclopedia\r\n",
      "Natural\r\n"
     ]
    }
   ],
   "source": [
    "!python q53.py | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 54. 品詞タグ付け\n",
    "Stanford Core NLPの解析結果XMLを読み込み，単語，レンマ，品詞をタブ区切り形式で出力せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing q54.py\n"
     ]
    }
   ],
   "source": [
    "%%file q54.py\n",
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse('work/nlp.txt.xml')\n",
    "root = tree.getroot()\n",
    "for token in root.iter('token'):\n",
    "    print(\"{}\\t{}\\t{}\".format(token[0].text, token[1].text, token[4].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural\tnatural\tJJ\r\n",
      "language\tlanguage\tNN\r\n",
      "processing\tprocessing\tNN\r\n",
      "From\tfrom\tIN\r\n",
      "Wikipedia\tWikipedia\tNNP\r\n",
      ",\t,\t,\r\n",
      "the\tthe\tDT\r\n",
      "free\tfree\tJJ\r\n",
      "encyclopedia\tencyclopedia\tNN\r\n",
      "Natural\tnatural\tJJ\r\n",
      "Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='UTF-8'>\r\n",
      "BrokenPipeError: [Errno 32] Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!python q54.py | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 55. 固有表現抽出\n",
    "入力文中の人名をすべて抜き出せ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing q55.py\n"
     ]
    }
   ],
   "source": [
    "%%file q55.py\n",
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse('work/nlp.txt.xml')\n",
    "root = tree.getroot()\n",
    "for token in root.findall(\".//*[NER='PERSON']\"): \n",
    "    print(token[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alan\r\n",
      "Turing\r\n",
      "Joseph\r\n",
      "Weizenbaum\r\n",
      "MARGIE\r\n",
      "Schank\r\n",
      "Wilensky\r\n",
      "Meehan\r\n",
      "Lehnert\r\n",
      "Carbonell\r\n"
     ]
    }
   ],
   "source": [
    "!python q55.py | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 56. 共参照解析\n",
    "Stanford Core NLPの共参照解析の結果に基づき，文中の参照表現（mention）を代表参照表現（representative mention）に置換せよ．ただし，置換するときは，「代表参照表現（参照表現）」のように，元の参照表現が分かるように配慮せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting q56.py\n"
     ]
    }
   ],
   "source": [
    "%%file q56.py\n",
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse('work/nlp.txt.xml')\n",
    "root = tree.getroot()\n",
    "for coref in root.findall('.//coreference/coreference'):\n",
    "    rep_mention = coref.find(\"*[@representative='true']\")\n",
    "    for mention in coref:\n",
    "        word_start, word_end = mention[-1].text.split()[0], mention[-1].text.split()[-1]\n",
    "        i_sent, i_start, i_end = mention[0].text, mention[1].text, str(int(mention[2].text)-1)\n",
    "        root.find(\".//sentence[@id='{}']/tokens/token[@id='{}']\".format(i_sent, i_start))[0].text \\\n",
    "        = \" [\" + rep_mention[-1].text + \"](\" + word_start\n",
    "        root.find(\".//sentence[@id='{}']/tokens/token[@id='{}']\".format(i_sent, i_end))[0].text \\\n",
    "        = word_end + \")\"\n",
    "tree.write('work/nlp_coreference.txt.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing From Wikipedia, [the free encyclopedia Natural language processing-LRB- NLP-RRB-](the free encyclopedia Natural language processing-LRB- NLP-RRB-) is [the free encyclopedia Natural language processing-LRB- NLP-RRB-](a field of computer science), artificial intelligence, and linguistics concerned with the interactions between computers) and human-LRB- natural-RRB- languages. \r\n",
      "As such, NLP is related to the area of humani-computer interaction. \r\n",
      "Many challenges in NLP involve natural language understanding, that is, enabling computers) to derive meaning from human or natural language input, and others involve natural language generation. \r\n",
      "History The history of NLP) generally starts in the 1950s, although work can be found from earlier periods. \r\n",
      "In 1950, [Alan Turing](Alan Turing) published an article titled`` Computing Machinery and Intelligence'' which proposed what is now called the Turing) test as a criterion of intelligence. \r\n",
      "The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. \r\n",
      "The authors claimed that within three or five years, [a solved problem](machine translation) would be [a solved problem](a solved problem). \r\n",
      "However, real progress was much slower, and after the ALPAC report in 1966, which found that ten year long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. \r\n",
      "Little further research in [a solved problem](machine translation) was conducted until [the late 1980s](the late 1980s), when the first statistical machine translation systems were developed. \r\n",
      "Some notably successful NLP systems developed in the 1960s were SHRDLU), [SHRDLU](a natural language system working in restricted`` blocks worlds'' with restricted vocabularies), and ELIZA), a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 to 1966. \r\n"
     ]
    }
   ],
   "source": [
    "#出力の整形\n",
    "import xml.etree.ElementTree as ET\n",
    "with open('work/nlp_coreferene.txt', 'w') as f:\n",
    "    tree = ET.parse('work/nlp_coreference.txt.xml')\n",
    "    root = tree.getroot()\n",
    "    for sentence in root.iter('sentence'):\n",
    "        for word in sentence.iter('word'):\n",
    "            f.write(word.text + \" \")\n",
    "        f.write(\"\\n\")\n",
    "! gsed -E \"s/\\s(\\W)/\\1/g\" work/nlp_coreferene.txt | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 57. 係り受け解析\n",
    "Stanford Core NLPの係り受け解析の結果（collapsed-dependencies）を有向グラフとして可視化せよ．可視化には，係り受け木をDOT言語に変換し，Graphvizを用いるとよい．また，Pythonから有向グラフを直接的に可視化するには，pydotを使うとよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing q57.py\n"
     ]
    }
   ],
   "source": [
    "%%file q57.py\n",
    "from graphviz import Digraph\n",
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse('work/nlp.txt.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "def dependency_tree(i_sent):\n",
    "    G = Digraph(format='png')\n",
    "    G.attr('node', shape='circle')\n",
    "    G.node(\"0\", \"ROOT\")\n",
    "    root = tree.getroot()\n",
    "    for dep in root.findall(\".//sentence[@id='{}']/dependencies[@type='collapsed-dependencies']/dep\".format(i_sent)):\n",
    "        G.node(dep[1].get('idx'), dep[1].text)\n",
    "        G.edge(dep[0].get('idx'), dep[1].get('idx'))\n",
    "    G.render(\"work/57\")\n",
    "    print(G)\n",
    "\n",
    "dependency_tree(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digraph {\r\n",
      "\tnode [shape=circle]\r\n",
      "\t0 [label=ROOT]\r\n",
      "\t6 [label=related]\r\n",
      "\t\t0 -> 6\r\n",
      "\t1 [label=As]\r\n",
      "\t\t2 -> 1\r\n",
      "\t2 [label=such]\r\n",
      "\t\t6 -> 2\r\n",
      "\t3 [label=\",\"]\r\n",
      "\t\t6 -> 3\r\n",
      "\t4 [label=NLP]\r\n",
      "\t\t6 -> 4\r\n",
      "\t5 [label=is]\r\n",
      "\t\t6 -> 5\r\n",
      "\t7 [label=to]\r\n",
      "\t\t9 -> 7\r\n",
      "\t8 [label=the]\r\n",
      "\t\t9 -> 8\r\n",
      "\t9 [label=area]\r\n",
      "\t\t6 -> 9\r\n",
      "\t10 [label=of]\r\n",
      "\t\t12 -> 10\r\n",
      "\t11 [label=\"humani-computer\"]\r\n",
      "\t\t12 -> 11\r\n",
      "\t12 [label=interaction]\r\n",
      "\t\t9 -> 12\r\n",
      "\t13 [label=\".\"]\r\n",
      "\t\t6 -> 13\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!python q57.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"work/57.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 58. タプルの抽出\n",
    "Stanford Core NLPの係り受け解析の結果（collapsed-dependencies）に基づき，「主語 述語 目的語」の組をタブ区切り形式で出力せよ．ただし，主語，述語，目的語の定義は以下を参考にせよ．\n",
    "\n",
    "述語: nsubj関係とdobj関係の子（dependant）を持つ単語\n",
    "主語: 述語からnsubj関係にある子（dependent）\n",
    "目的語: 述語からdobj関係にある子（dependent）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting q58.py\n"
     ]
    }
   ],
   "source": [
    "%%file q58.py\n",
    "import xml.etree.ElementTree as ET\n",
    "def dep_nd():\n",
    "    tree = ET.parse('work/nlp.txt.xml')\n",
    "    root = tree.getroot()\n",
    "    dep_n = [(dep_nsubj[0], dep_nsubj[1]) \n",
    "     for dep_nsubj in root.findall(\".//dependencies[@type='collapsed-dependencies']/dep[@type='nsubj']\")]\n",
    "    dep_d = [(dep_nsubj[0], dep_nsubj[1]) \n",
    "     for dep_nsubj in root.findall(\".//dependencies[@type='collapsed-dependencies']/dep[@type='dobj']\")]\n",
    "    make_tuple(dep_n, dep_d)\n",
    "\n",
    "def make_tuple(dep_n, dep_d):\n",
    "    for n in dep_n:\n",
    "        for s in dep_d:\n",
    "            if n[0].get('id') == s[0].get('id'):\n",
    "                print(n[1].text, n[0].text, s[1].text)\n",
    "\n",
    "dep_nd()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing field computers\r\n",
      "processing field meaning\r\n",
      "processing field generation\r\n",
      "processing field article\r\n",
      "processing field test\r\n",
      "processing field translation\r\n",
      "processing field expectations\r\n",
      "processing field information\r\n",
      "processing field interaction\r\n",
      "processing field base\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"q58.py\", line 17, in <module>\r\n",
      "    dep_nd()   \r\n",
      "  File \"q58.py\", line 9, in dep_nd\r\n",
      "    make_tuple(dep_n, dep_d)\r\n",
      "  File \"q58.py\", line 15, in make_tuple\r\n",
      "    print(n[1].text, n[0].text, s[1].text)\r\n",
      "BrokenPipeError: [Errno 32] Broken pipe\r\n",
      "Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='UTF-8'>\r\n",
      "BrokenPipeError: [Errno 32] Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!python q58.py | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 59. S式の解析\n",
    "Stanford Core NLPの句構造解析の結果（S式）を読み込み，文中のすべての名詞句（NP）を表示せよ．入れ子になっている名詞句もすべて表示すること."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "S式のパーサー(ネットで見つけた)を利用\n",
    "http://www.unixuser.org/~euske/python/index-j.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting q59.py\n"
     ]
    }
   ],
   "source": [
    "%%file q59.py\n",
    "import xml.etree.ElementTree as ET\n",
    "import sexpr\n",
    "def parse():\n",
    "    tree = ET.parse('work/nlp.txt.xml')\n",
    "    root = tree.getroot()\n",
    "    for s in root.findall(\".//parse\"):\n",
    "        list_s = sexpr.str2sexpr(s.text)\n",
    "        search_np(list_s)\n",
    "\n",
    "def search_np(list_s):\n",
    "    if len(list_s) == 1:\n",
    "        search_np(list_s[0])\n",
    "    elif list_s[0] == \"NP\":\n",
    "        print_np(list_s)\n",
    "        for element in list_s[1:]:\n",
    "            search_np(element)\n",
    "        print()\n",
    "    elif isinstance(list_s[1], list):\n",
    "        for element in list_s[1:]:\n",
    "            search_np(element)\n",
    "    return\n",
    "\n",
    "def print_np(list_s):\n",
    "    for element_np in list_s[1:]:\n",
    "        if isinstance(element_np[1], str):\n",
    "            print(element_np[1], end=\" \")\n",
    "        else:\n",
    "            print_np(element_np)\n",
    "\n",
    "parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing \r\n",
      "Wikipedia \r\n",
      "the free encyclopedia Natural language processing -LRB- NLP -RRB- the free encyclopedia Natural language processing \r\n",
      "NLP \r\n",
      "\r\n",
      "a field of computer science , artificial intelligence , and linguistics concerned with the interactions between computers and human -LRB- natural -RRB- languages a field of computer science a field \r\n",
      "computer science \r\n",
      "\r\n",
      "artificial intelligence \r\n",
      "linguistics concerned with the interactions between computers and human -LRB- natural -RRB- languages linguistics \r\n",
      "Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='UTF-8'>\r\n",
      "BrokenPipeError: [Errno 32] Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!python q59.py | head"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
