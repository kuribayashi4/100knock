Natural 
language 
processing 
is 
a 
field 
of 
computer 
artificial 
and 
linguistics 
concerned 
with 
the 
interactions 
between 
computers 
and 
human 
languages

As 
NLP 
is 
related 
to 
the 
area 
of 
computer 
interaction

The 
history 
of 
NLP 
generally 
starts 
in 
the 
although 
work 
can 
be 
found 
from 
earlier 
periods

The 
Georgetown 
experiment 
in 
involved 
fully 
automatic 
translation 
of 
more 
than 
sixty 
Russian 
sentences 
into 
English

The 
authors 
claimed 
that 
within 
three 
or 
five 
machine 
translation 
would 
be 
a 
solved 
problem

real 
progress 
was 
much 
and 
after 
the 
ALPAC 
report 
in 
which 
found 
that 
ten 
year 
long 
research 
had 
failed 
to 
fulfill 
the 
funding 
for 
machine 
translation 
was 
dramatically 
reduced

Some 
notably 
successful 
NLP 
systems 
developed 
in 
the 
s 
were 
a 
natural 
language 
system 
working 
in 
restricted 
blocks 
with 
restricted 
and 
a 
simulation 
of 
a 
Rogerian 
written 
by 
Joseph 
Weizenbaum 
between 
to 
Using 
almost 
no 
information 
about 
human 
thought 
or 
ELIZA 
sometimes 
provided 
a 
startlingly 
like 
interaction

During 
the 
s 
many 
programmers 
began 
to 
write 
conceptual 
which 
structured 
world 
information 
into 
understandable 
data

Examples 
are 
MARGIE 
SAM 
PAM 
TaleSpin 
QUALM 
Politics 
and 
Plot 
Units 
Lehnert 
Up 
to 
the 
most 
NLP 
systems 
were 
based 
on 
complex 
sets 
of 
written 
rules

Starting 
in 
the 
late 
there 
was 
a 
revolution 
in 
NLP 
with 
the 
introduction 
of 
machine 
learning 
algorithms 
for 
language 
processing

This 
was 
due 
to 
both 
the 
steady 
increase 
in 
computational 
power 
resulting 
from 
s 
Law 
and 
the 
gradual 
lessening 
of 
the 
dominance 
of 
Chomskyan 
theories 
of 
linguistics 
e

g

transformational 
whose 
theoretical 
underpinnings 
discouraged 
the 
sort 
of 
corpus 
linguistics 
that 
underlies 
the 
learning 
approach 
to 
language 
processing

Some 
of 
the 
used 
machine 
learning 
such 
as 
decision 
produced 
systems 
of 
hard 
then 
rules 
similar 
to 
existing 
written 
rules

Part 
of 
speech 
tagging 
introduced 
the 
use 
of 
Hidden 
Markov 
Models 
to 
and 
research 
has 
focused 
on 
statistical 
which 
make 
probabilistic 
decisions 
based 
on 
attaching 
valued 
weights 
to 
the 
features 
making 
up 
the 
input 
data

The 
cache 
language 
models 
upon 
which 
many 
speech 
recognition 
systems 
now 
rely 
are 
examples 
of 
such 
statistical 
models

Many 
of 
the 
notable 
early 
successes 
occurred 
in 
the 
field 
of 
machine 
due 
especially 
to 
work 
at 
IBM 
where 
successively 
more 
complicated 
statistical 
models 
were 
developed

These 
systems 
were 
able 
to 
take 
advantage 
of 
existing 
multilingual 
textual 
corpora 
that 
had 
been 
produced 
by 
the 
Parliament 
of 
Canada 
and 
the 
European 
Union 
as 
a 
result 
of 
laws 
calling 
for 
the 
translation 
of 
all 
governmental 
proceedings 
into 
all 
official 
languages 
of 
the 
corresponding 
systems 
of 
government

most 
other 
systems 
depended 
on 
corpora 
specifically 
developed 
for 
the 
tasks 
implemented 
by 
these 
which 
was 
and 
often 
continues 
to 
a 
major 
limitation 
in 
the 
success 
of 
these 
systems

Recent 
research 
has 
increasingly 
focused 
on 
unsupervised 
and 
supervised 
learning 
algorithms

Such 
algorithms 
are 
able 
to 
learn 
from 
data 
that 
has 
not 
been 
annotated 
with 
the 
desired 
or 
using 
a 
combination 
of 
annotated 
and 
annotated 
data

this 
task 
is 
much 
more 
difficult 
than 
supervised 
and 
typically 
produces 
less 
accurate 
results 
for 
a 
given 
amount 
of 
input 
data

Modern 
NLP 
algorithms 
are 
based 
on 
machine 
especially 
statistical 
machine 
learning

The 
paradigm 
of 
machine 
learning 
is 
different 
from 
that 
of 
most 
prior 
attempts 
at 
language 
processing

Prior 
implementations 
of 
processing 
tasks 
typically 
involved 
the 
direct 
hand 
coding 
of 
large 
sets 
of 
rules

The 
learning 
paradigm 
calls 
instead 
for 
using 
general 
learning 
algorithms 
although 
not 
grounded 
in 
statistical 
inference 
to 
automatically 
learn 
such 
rules 
through 
the 
analysis 
of 
large 
corpora 
of 
typical 
world 
examples

Many 
different 
classes 
of 
machine 
learning 
algorithms 
have 
been 
applied 
to 
NLP 
tasks

These 
algorithms 
take 
as 
input 
a 
large 
set 
of 
that 
are 
generated 
from 
the 
input 
data

Some 
of 
the 
used 
such 
as 
decision 
produced 
systems 
of 
hard 
then 
rules 
similar 
to 
the 
systems 
of 
written 
rules 
that 
were 
then 
common

research 
has 
focused 
on 
statistical 
which 
make 
probabilistic 
decisions 
based 
on 
attaching 
valued 
weights 
to 
each 
input 
feature

Automatic 
learning 
procedures 
can 
make 
use 
of 
statistical 
inference 
algorithms 
to 
produce 
models 
that 
are 
robust 
to 
unfamiliar 
input 
e

g

containing 
words 
or 
structures 
that 
have 
not 
been 
seen 
and 
to 
erroneous 
input 
e

g

with 
misspelled 
words 
or 
words 
accidentally 
Systems 
based 
on 
automatically 
learning 
the 
rules 
can 
be 
made 
more 
accurate 
simply 
by 
supplying 
more 
input 
data

systems 
based 
on 
written 
rules 
can 
only 
be 
made 
more 
accurate 
by 
increasing 
the 
complexity 
of 
the 
which 
is 
a 
much 
more 
difficult 
task

In 
there 
is 
a 
limit 
to 
the 
complexity 
of 
systems 
based 
on 
crafted 
beyond 
which 
the 
systems 
become 
more 
and 
more 
unmanageable

The 
subfield 
of 
NLP 
devoted 
to 
learning 
approaches 
is 
known 
as 
Natural 
Language 
Learning 
and 
its 
conference 
CoNLL 
and 
peak 
body 
SIGNLL 
are 
sponsored 
by 
recognizing 
also 
their 
links 
with 
Computational 
Linguistics 
and 
Language 
Acquisition

