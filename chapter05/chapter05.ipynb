{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 5章: 係り受け解析\n",
    "夏目漱石の小説『吾輩は猫である』の文章（neko.txt）をCaboChaを使って係り受け解析し，その結果をneko.txt.cabochaというファイルに保存せよ．このファイルを用いて，以下の問に対応するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!cabocha -f1 < data/neko.txt >work/neko.txt.cabocha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 目次\n",
    "* [40. 係り受け解析結果の読み込み（形態素）](#sec1)\n",
    "* [41. 係り受け解析結果の読み込み（文節・係り受け）](#sec2)\n",
    "* [42. 係り元と係り先の文節の表示](#sec3)\n",
    "* [43. 名詞を含む文節が動詞を含む文節に係るものを抽出](#sec4)\n",
    "* [44. 係り受け木の可視化](#sec5)\n",
    "* [45. 動詞の格パターンの抽出](#sec6)\n",
    "* [46. 動詞の格フレーム情報の抽出](#sec7)\n",
    "* [47. 機能動詞構文のマイニング](#sec8)\n",
    "* [48. 名詞から根へのパスの抽出](#sec9)\n",
    "* [49. 名詞間の係り受けパスの抽出](#sec10)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 0 -1D 0/0 0.000000\r\n",
      "一\t名詞,数,*,*,*,*,一,イチ,イチ\r\n",
      "EOS\r\n",
      "EOS\r\n",
      "* 0 2D 0/0 -0.764522\r\n",
      "　\t記号,空白,*,*,*,*,　,　,　\r\n",
      "* 1 2D 0/1 -0.764522\r\n",
      "吾輩\t名詞,代名詞,一般,*,*,*,吾輩,ワガハイ,ワガハイ\r\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\r\n",
      "* 2 -1D 0/2 0.000000\r\n",
      "猫\t名詞,一般,*,*,*,*,猫,ネコ,ネコ\r\n",
      "で\t助動詞,*,*,*,特殊・ダ,連用形,だ,デ,デ\r\n",
      "ある\t助動詞,*,*,*,五段・ラ行アル,基本形,ある,アル,アル\r\n",
      "。\t記号,句点,*,*,*,*,。,。,。\r\n",
      "EOS\r\n",
      "* 0 2D 0/1 -1.911675\r\n",
      "名前\t名詞,一般,*,*,*,*,名前,ナマエ,ナマエ\r\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\r\n",
      "* 1 2D 0/0 -1.911675\r\n",
      "まだ\t副詞,助詞類接続,*,*,*,*,まだ,マダ,マダ\r\n",
      "* 2 -1D 0/0 0.000000\r\n",
      "無い\t形容詞,自立,*,*,形容詞・アウオ段,基本形,無い,ナイ,ナイ\r\n",
      "。\t記号,句点,*,*,*,*,。,。,。\r\n",
      "EOS\r\n",
      "EOS\r\n",
      "* 0 1D 1/2 1.058678\r\n",
      "　\t記号,空白,*,*,*,*,　,　,　\r\n",
      "どこ\t名詞,代名詞,一般,*,*,*,どこ,ドコ,ドコ\r\n",
      "で\t助詞,格助詞,一般,*,*,*,で,デ,デ\r\n",
      "* 1 4D 0/2 -1.453749\r\n"
     ]
    }
   ],
   "source": [
    "!head -n30 ../work/neko.txt.cabocha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 40. 係り受け解析結果の読み込み（形態素）<a id=\"sec1\"></a>\n",
    "形態素を表すクラスMorphを実装せよ．このクラスは表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をメンバ変数に持つこととする．さらに，CaboChaの解析結果（neko.txt.cabocha）を読み込み，各文をMorphオブジェクトのリストとして表現し，3文目の形態素列を表示せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "インスタンス変数は後からバンバン定義できる  \n",
    "slotsでインスタンス変数名を  \n",
    "enum Cのenum  \n",
    "classを単にメソッドのまとめ上げとして使うのは勿体無い(わかりにくい)  \n",
    "Classを継承する際はhas a関係を意識  \n",
    "Enum 値に名前をつけられる  \n",
    "str.replaceは数が分かっている場合は指定した方が早い  \n",
    "__init__のデフォルト引数にmutableな変数を渡すのはやめた方が良い  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "[[]]*3 要素の複製なので、mutableを共有している(どれか変更したら全て変更される)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "parse関数を切り出す  \n",
    "Morphのクラスメソッドとしてリストを作る関数を定義  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing q40.py\n"
     ]
    }
   ],
   "source": [
    "%%file q40.py\n",
    "from itertools import groupby\n",
    "from cytoolz import nth\n",
    "file_path = 'work/neko.txt.cabocha'\n",
    "class Morph:\n",
    "    def __init__(self, surface, base, pos, pos1): \n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"{}\\t{}, {}, {}\".format(self.surface, self.base, self.pos, self.pos1)\n",
    "\n",
    "    @classmethod\n",
    "    def sent2morph(cls, sent_block): #文をmorphオブジェクトのリストに\n",
    "        sentence = []\n",
    "        for line in sent_block:\n",
    "            if not line.startswith(\"* \"):\n",
    "                details = line.rstrip().replace(\"\\t\", \",\").split(\",\")\n",
    "                sentence.append(Morph(details[0], details[7], details[1], details[2]))\n",
    "        return sentence\n",
    "    \n",
    "def parse():\n",
    "    with open(file_path, 'r') as f:\n",
    "        for is_eos, sent_block in groupby(f, lambda x: x.strip() == \"EOS\"): #文単位に分ける\n",
    "            if not is_eos:\n",
    "                yield Morph.sent2morph(sent_block)    \n",
    "                \n",
    "def main():\n",
    "    neko = parse()\n",
    "    for morph in nth(3, neko):\n",
    "        print(morph)\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　\t　, 記号, 空白\r\n",
      "どこ\tどこ, 名詞, 代名詞\r\n",
      "で\tで, 助詞, 格助詞\r\n",
      "生れ\t生れる, 動詞, 自立\r\n",
      "た\tた, 助動詞, *\r\n",
      "か\tか, 助詞, 副助詞／並立助詞／終助詞\r\n",
      "とんと\tとんと, 副詞, 一般\r\n",
      "見当\t見当, 名詞, サ変接続\r\n",
      "が\tが, 助詞, 格助詞\r\n",
      "つか\tつく, 動詞, 自立\r\n",
      "ぬ\tぬ, 助動詞, *\r\n",
      "。\t。, 記号, 句点\r\n"
     ]
    }
   ],
   "source": [
    "!python q40.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 41. 係り受け解析結果の読み込み（文節・係り受け）<a id=\"sec2\"></a>\n",
    "40に加えて，文節を表すクラスChunkを実装せよ．このクラスは形態素（Morphオブジェクト）のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする．さらに，入力テキストのCaboChaの解析結果を読み込み，１文をChunkオブジェクトのリストとして表現し，8文目の文節の文字列と係り先を表示せよ．第5章の残りの問題では，ここで作ったプログラムを活用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting q41.py\n"
     ]
    }
   ],
   "source": [
    "%%file q41.py\n",
    "from q40 import Morph\n",
    "from itertools import groupby\n",
    "from cytoolz import nth\n",
    "file_path = 'work/neko.txt.cabocha'\n",
    "\n",
    "class Chunk(Morph):\n",
    "    def __init__(self): \n",
    "        self.morphs = []\n",
    "        self.dst = -1\n",
    "        self.srcs = []\n",
    "        self.index = 0\n",
    "\n",
    "    def details(self): #結果出力用\n",
    "        return (\"<chunk{}> 係り先:{}, 係り元:\" \n",
    "                + \" \".join(str(index) for index in self.srcs) + \"\\n\" \n",
    "                + \"\\n\".join(str(morph) for morph in self.morphs) + \"\\n\").format(self.index, self.dst)\n",
    "\n",
    "    def __str__(self): #文節の文字列を返す\n",
    "        return (\"\".join(morph.surface for morph in self.morphs if morph.pos != \"記号\")).strip()\n",
    "    \n",
    "    def surface_q49(self, alphabet): #名詞をalphabetに置き換えて表示(q49で利用)\n",
    "        chunk_surface = \"\"\n",
    "        for index_morph in range(len(self.morphs)):\n",
    "            if self.morphs[index_morph].pos != \"記号\":\n",
    "                if self.morphs[index_morph].pos == \"名詞\":\n",
    "                    chunk_surface = chunk_surface + alphabet\n",
    "                else:\n",
    "                    chunk_surface = chunk_surface + self.morphs[index_morph].surface\n",
    "        return chunk_surface\n",
    "                                                            \n",
    "    @classmethod\n",
    "    def _dependency(cls, sentence): #係り元の文節のindexを探す\n",
    "        for chunk in sentence:\n",
    "            if chunk.dst > -1:\n",
    "                sentence[chunk.dst].srcs.append(chunk.index)\n",
    "            \n",
    "    @classmethod\n",
    "    def sent2chunk(cls, sent_block): #文をchunkのリストに\n",
    "        sentence = []\n",
    "        for is_chunk, chunk_or_morphs in groupby(sent_block, lambda x: x.startswith(\"*\")):\n",
    "            if is_chunk:\n",
    "                details = list(chunk_or_morphs)[0].lstrip(\"*\").strip().split()\n",
    "                chunk = Chunk()\n",
    "                chunk.dst = int(details[1].rstrip(\"D\"))\n",
    "                chunk.index = int(details[0])\n",
    "            if not is_chunk:\n",
    "                chunk.morphs = Morph.sent2morph(chunk_or_morphs) #親のメソッドで形態素のリストに\n",
    "                sentence.append(chunk)\n",
    "        cls._dependency(sentence)\n",
    "        return sentence\n",
    "\n",
    "def parse():\n",
    "    with open(file_path, 'r') as f:\n",
    "        for is_eos, sent_block in groupby(f, lambda x: x.strip() == \"EOS\"): #文単位に分ける\n",
    "            if not is_eos:\n",
    "                yield Chunk.sent2chunk(sent_block)\n",
    "def main():\n",
    "    neko = parse()\n",
    "    for chunk in nth(7, neko):\n",
    "        print(chunk.details())\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<chunk0> 係り先:1, 係り元:\r\n",
      "この\tこの, 連体詞, *\r\n",
      "\r\n",
      "<chunk1> 係り先:7, 係り元:0\r\n",
      "書生\t書生, 名詞, 一般\r\n",
      "という\tという, 助詞, 格助詞\r\n",
      "の\tの, 名詞, 非自立\r\n",
      "は\tは, 助詞, 係助詞\r\n",
      "\r\n",
      "<chunk2> 係り先:4, 係り元:\r\n",
      "時々\t時々, 副詞, 一般\r\n",
      "\r\n",
      "<chunk3> 係り先:4, 係り元:\r\n",
      "我々\t我々, 名詞, 代名詞\r\n",
      "を\tを, 助詞, 格助詞\r\n",
      "\r\n",
      "<chunk4> 係り先:5, 係り元:2 3\r\n",
      "捕え\t捕える, 動詞, 自立\r\n",
      "て\tて, 助詞, 接続助詞\r\n",
      "\r\n",
      "<chunk5> 係り先:6, 係り元:4\r\n",
      "煮\t煮る, 動詞, 自立\r\n",
      "て\tて, 助詞, 接続助詞\r\n",
      "\r\n",
      "<chunk6> 係り先:7, 係り元:5\r\n",
      "食う\t食う, 動詞, 自立\r\n",
      "という\tという, 助詞, 格助詞\r\n",
      "\r\n",
      "<chunk7> 係り先:-1, 係り元:1 6\r\n",
      "話\t話, 名詞, サ変接続\r\n",
      "で\tだ, 助動詞, *\r\n",
      "ある\tある, 助動詞, *\r\n",
      "。\t。, 記号, 句点\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python q41.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 42. 係り元と係り先の文節の表示<a id=\"sec3\"></a>\n",
    "係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting q42.py\n"
     ]
    }
   ],
   "source": [
    "%%file q42.py\n",
    "from itertools import islice\n",
    "from cytoolz import nth\n",
    "from q41 import parse\n",
    "\n",
    "def depend():\n",
    "    for sentence in parse():\n",
    "        sent_depend = []\n",
    "        for chunk in sentence:\n",
    "            if str(chunk) and chunk.dst != -1:\n",
    "                sent_depend.append(str(chunk) + \"\\t\" + str(sentence[chunk.dst]))\n",
    "        yield sent_depend \n",
    "\n",
    "def main():\n",
    "    for sent_depend in islice(depend(),7,8):\n",
    "        for dependency in sent_depend:\n",
    "            print(dependency)\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "この\t書生というのは\r\n",
      "書生というのは\t話である\r\n",
      "時々\t捕えて\r\n",
      "我々を\t捕えて\r\n",
      "捕えて\t煮て\r\n",
      "煮て\t食うという\r\n",
      "食うという\t話である\r\n"
     ]
    }
   ],
   "source": [
    "!python q42.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 43. 名詞を含む文節が動詞を含む文節に係るものを抽出<a id=\"sec4\"></a>\n",
    "名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting q43.py\n"
     ]
    }
   ],
   "source": [
    "%%file q43.py\n",
    "from itertools import islice\n",
    "from q41 import parse\n",
    "\n",
    "def is_pos(chunk, pos):\n",
    "    for morph in chunk.morphs:\n",
    "        if morph.pos == pos:\n",
    "            return True\n",
    "        \n",
    "def depend():\n",
    "    for sentence in parse():\n",
    "        sent_depend = []\n",
    "        for chunk in sentence:\n",
    "            if all((str(chunk), chunk.dst != -1,\n",
    "                    is_pos(chunk, \"名詞\"), is_pos(sentence[chunk.dst], \"動詞\"))):\n",
    "                sent_depend.append(str(chunk) + \"\\t\" + str(sentence[chunk.dst]))\n",
    "        yield sent_depend \n",
    "\n",
    "def main():\n",
    "    for sent_depend in islice(depend(),0,8):\n",
    "        for dependency in sent_depend:\n",
    "            print(dependency)\n",
    "\n",
    "if __name__ ==  \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "どこで\t生れたか\r\n",
      "見当が\tつかぬ\r\n",
      "所で\t泣いて\r\n",
      "ニャーニャー\t泣いて\r\n",
      "いた事だけは\t記憶している\r\n",
      "吾輩は\t見た\r\n",
      "ここで\t始めて\r\n",
      "ものを\t見た\r\n",
      "あとで\t聞くと\r\n",
      "我々を\t捕えて\r\n"
     ]
    }
   ],
   "source": [
    "!python q43.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 44. 係り受け木の可視化<a id=\"sec5\"></a>\n",
    "与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，係り受け木をDOT言語に変換し，Graphvizを用いるとよい．また，Pythonから有向グラフを直接的に可視化するには，pydotを使うとよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing q44.py\n"
     ]
    }
   ],
   "source": [
    "%%file q44.py\n",
    "from graphviz import Digraph\n",
    "from q41 import parse\n",
    "from cytoolz import nth\n",
    "\n",
    "def make_tree(sentence):\n",
    "    G = Digraph(format='png')\n",
    "    G.attr('node', shape='circle')\n",
    "    for chunk in sentence:\n",
    "        surf_chunk = str(chunk)\n",
    "        if surf_chunk: \n",
    "            G.node(str(chunk.index), surf_chunk)\n",
    "            if chunk.dst != -1: \n",
    "                G.edge(str(chunk.index), str(chunk.dst))\n",
    "    G.render('work/44')\n",
    "    print(G)\n",
    "\n",
    "def main():\n",
    "    make_tree(nth(3, parse()))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digraph {\r\n",
      "\tnode [shape=circle]\r\n",
      "\t0 [label=\"どこで\"]\r\n",
      "\t\t0 -> 1\r\n",
      "\t1 [label=\"生れたか\"]\r\n",
      "\t\t1 -> 4\r\n",
      "\t2 [label=\"とんと\"]\r\n",
      "\t\t2 -> 4\r\n",
      "\t3 [label=\"見当が\"]\r\n",
      "\t\t3 -> 4\r\n",
      "\t4 [label=\"つかぬ\"]\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!python q44.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#<img src=\"../work/44.png\"> で表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<img src=\"work/44.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 45. 動詞の格パターンの抽出<a id=\"sec6\"></a>\n",
    "今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい． 動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ． ただし，出力は以下の仕様を満たすようにせよ．\n",
    "\n",
    "* 動詞を含む文節において，最左の動詞の基本形を述語とする\n",
    "* 述語に係る助詞を格とする\n",
    "* 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる  \n",
    "\n",
    "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "始める  で  \n",
    "見る    は を  \n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "\n",
    "* コーパス中で頻出する述語と格パターンの組み合わせ\n",
    "* 「する」「見る」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting q45.py\n"
     ]
    }
   ],
   "source": [
    "%%file q45.py\n",
    "from itertools import chain\n",
    "from q41 import parse\n",
    "\n",
    "class VerbFrame:\n",
    "    @classmethod\n",
    "    def lget_verb(cls, chunk):\n",
    "        for morph in chunk.morphs:\n",
    "            if morph.pos == \"動詞\":\n",
    "                return morph.base\n",
    "        return False \n",
    "    \n",
    "    @classmethod\n",
    "    def get_zyosi(cls, chunk):\n",
    "        list_zyosi = []\n",
    "        for morph in chunk.morphs:\n",
    "            if morph.pos == \"助詞\":\n",
    "                list_zyosi.append(morph.base)\n",
    "        return list_zyosi\n",
    "    \n",
    "    @classmethod\n",
    "    def make_data(cls, chunk, sentence):\n",
    "        return ([cls.lget_verb(chunk),[cls.get_zyosi(sentence[index_srcs]) \n",
    "                                       for index_srcs in chunk.srcs]])\n",
    "    \n",
    "    @classmethod\n",
    "    def make_frame(cls):\n",
    "        for sentence in parse():\n",
    "            for chunk in sentence:\n",
    "                if cls.lget_verb(chunk):\n",
    "                    yield cls.make_data(chunk, sentence)\n",
    "def main():\n",
    "    with open(\"work/45.txt\", \"w\") as f:\n",
    "        for predicate in VerbFrame.make_frame():\n",
    "            cases = \" \".join(list(chain.from_iterable(predicate[1])))\n",
    "            if cases:\n",
    "                f.write(predicate[0] + \"\\t\" + cases + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!python q45.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生れる\tで\r\n",
      "つく\tか が\r\n",
      "泣く\tで\r\n",
      "する\tて だけ は\r\n",
      "始める\tで\r\n",
      "見る\tは を\r\n",
      "聞く\tで\r\n",
      "捕える\tを\r\n",
      "煮る\tて\r\n",
      "食う\tて\r\n"
     ]
    }
   ],
   "source": [
    "!head -n10 work/45.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 565 云う\tと\r\n",
      " 442 する\tを\r\n",
      " 249 思う\tと\r\n",
      " 199 ある\tが\r\n",
      " 189 なる\tに\r\n",
      " 174 する\tに\r\n",
      " 173 見る\tて\r\n",
      " 127 する\tと\r\n",
      " 117 する\tが\r\n",
      "  94 見る\tを\r\n"
     ]
    }
   ],
   "source": [
    "!cat work/45.txt | sort | uniq -c | sort -r 2>/dev/null | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 442 する\tを\r\n",
      " 174 する\tに\r\n",
      " 127 する\tと\r\n",
      " 117 する\tが\r\n",
      "  84 する\tて を\r\n",
      "  59 する\tは\r\n",
      "  58 する\tを に\r\n",
      "  58 する\tて\r\n",
      "  51 する\tが を\r\n",
      "  48 する\tから\r\n"
     ]
    }
   ],
   "source": [
    "!cat work/45.txt | sort | uniq -c | sort -r | grep \"する\" 2>/dev/null | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 173 見る\tて\r\n",
      "  94 見る\tを\r\n",
      "  21 見る\tて て\r\n",
      "  20 見る\tから\r\n",
      "  16 見る\tて を\r\n",
      "  14 見る\tと\r\n",
      "  12 見る\tで\r\n",
      "  11 見る\tから て\r\n",
      "  11 見る\tは て\r\n",
      "   8 見る\tに\r\n"
     ]
    }
   ],
   "source": [
    "!cat work/45.txt | sort | uniq -c | sort -r | grep \"見る\" 2>/dev/null | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3 与える\tに を\r\n",
      "   1 与える\tけれども に は を\r\n",
      "   1 与える\tじゃあ か と は て を\r\n",
      "   1 与える\tとして を か\r\n",
      "   1 与える\tたり て に を\r\n",
      "   1 与える\tで だけ に を\r\n",
      "   1 与える\tに は に対して のみ は も\r\n",
      "   1 与える\tて が は は と て に を\r\n",
      "   1 与える\tは て に を に\r\n",
      "   1 与える\tは て に を\r\n"
     ]
    }
   ],
   "source": [
    "!cat work/45.txt | sort | uniq -c | sort -r | grep \"与える\" 2>/dev/null | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 46. 動詞の格フレーム情報の抽出<a id=\"sec7\"></a>\n",
    "45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．\n",
    "\n",
    "* 項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n",
    "* 述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる  \n",
    "  \n",
    "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである.  \n",
    "  \n",
    "始める  で      ここで  \n",
    "見る    は を   吾輩は ものを  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting q46.py\n"
     ]
    }
   ],
   "source": [
    "%%file q46.py\n",
    "from itertools import chain\n",
    "from q45 import VerbFrame\n",
    "\n",
    "class VerbFrame2(VerbFrame):    \n",
    "    @classmethod\n",
    "    def get_chunk(cls, chunk):\n",
    "        list_chunk = []\n",
    "        for morph in chunk.morphs:\n",
    "            if morph.pos == \"助詞\": \n",
    "                list_chunk.append(str(chunk))\n",
    "        return list_chunk\n",
    "    \n",
    "    @classmethod\n",
    "    def make_data(cls, chunk, sentence):\n",
    "        return ([cls.lget_verb(chunk), \n",
    "                 [cls.get_zyosi(sentence[index_srcs]) for index_srcs in chunk.srcs],\n",
    "                    [cls.get_chunk(sentence[index_srcs]) for index_srcs in chunk.srcs]])\n",
    "\n",
    "def main():\n",
    "    with open(\"work/46.txt\", \"w\") as f:\n",
    "        for predicate in VerbFrame2.make_frame():\n",
    "            cases = \" \".join(list(chain.from_iterable(predicate[1])))\n",
    "            chunks = \" \".join(list(chain.from_iterable(predicate[2])))\n",
    "            if cases: \n",
    "                f.write(predicate[0] + \"\\t\" + cases + \" \" + chunks + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!python q46.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生れる\tで どこで\r\n",
      "つく\tか が 生れたか 見当が\r\n",
      "泣く\tで 所で\r\n",
      "する\tて だけ は 泣いて いた事だけは いた事だけは\r\n",
      "始める\tで ここで\r\n",
      "見る\tは を 吾輩は ものを\r\n",
      "聞く\tで あとで\r\n",
      "捕える\tを 我々を\r\n",
      "煮る\tて 捕えて\r\n",
      "食う\tて 煮て\r\n"
     ]
    }
   ],
   "source": [
    "!head -n10 work/46.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 47. 機能動詞構文のマイニング<a id=\"sec8\"></a>\n",
    "動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい．46のプログラムを以下の仕様を満たすように改変せよ．   \n",
    "\n",
    "* 「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする\n",
    "* 述語は「サ変接続名詞+を+動詞の基本形」とし，文節中に複数の動詞があるときは，最左の動詞を用いる\n",
    "* 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "* 述語に係る文節が複数ある場合は，すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）  \n",
    "例えば「別段くるにも及ばんさと、主人は手紙に返事をする。」という文から，以下の出力が得られるはずである．\n",
    "  \n",
    "返事をする      と に は        及ばんさと 手紙に 主人は  \n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．  \n",
    "  \n",
    "* コーパス中で頻出する述語（サ変接続名詞+を+動詞）\n",
    "* コーパス中で頻出する述語と助詞パターン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing q47.py\n"
     ]
    }
   ],
   "source": [
    "%%file q47.py\n",
    "from itertools import chain\n",
    "from q45 import VerbFrame\n",
    "\n",
    "class VerbFrame3(VerbFrame): \n",
    "    @classmethod\n",
    "    def get_zyosi(cls, chunk, sentence):\n",
    "        list_zyosi = []\n",
    "        list_chunk = []\n",
    "        for index_chunk in chunk.srcs:\n",
    "            morphs = sentence[index_chunk].morphs\n",
    "            for morph_i in range(len(morphs) - 1):\n",
    "                if morphs[morph_i].pos1 == \"サ変接続\" and  morphs[morph_i + 1].base == \"を\":\n",
    "                    list_zyosi.append(\"を\")\n",
    "                    list_chunk.append(morphs[morph_i].surface + \"を\")\n",
    "        if list_zyosi == []:\n",
    "            return \"\"\n",
    "        else:\n",
    "            list_zyosi.append(list_chunk)\n",
    "            return list_zyosi\n",
    "    \n",
    "    @classmethod\n",
    "    def make_data(cls, chunk, sentence):\n",
    "        return ([cls.lget_verb(chunk), cls.get_zyosi(chunk, sentence)])\n",
    "\n",
    "def main():\n",
    "    with open(\"work/47.txt\", \"w\") as f:\n",
    "        for predicate in VerbFrame3.make_frame():\n",
    "            cases = \" \".join(list(chain.from_iterable(predicate[1])))\n",
    "            if cases:\n",
    "                f.write(predicate[0] + \"\\t\" + cases + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!python q47.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "する\tを 決心を\r\n",
      "する\tを 返報を\r\n",
      "する\tを 昼寝を\r\n",
      "する\tを 昼寝を\r\n",
      "加える\tを 迫害を\r\n",
      "する\tを 生活を\r\n",
      "する\tを 話を\r\n",
      "する\tを 投書を\r\n",
      "する\tを 話を\r\n",
      "する\tを 写生を\r\n",
      "する\tを 昼寝を\r\n",
      "見る\tを 彩色を\r\n",
      "する\tを 欠伸を\r\n",
      "する\tを 報道を\r\n",
      "する\tを 挨拶を\r\n",
      "食う\tを 御馳走を\r\n",
      "する\tを 問答を\r\n",
      "する\tを 雑談を\r\n",
      "する\tを 自慢を\r\n",
      "飲み込む\tを 呼吸を\r\n"
     ]
    }
   ],
   "source": [
    "!head -n20 work/47.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 374 する\r\n",
      "  21 やる\r\n",
      "  18 聞く\r\n",
      "  14 受ける\r\n",
      "  11 願う\r\n",
      "  10 始める\r\n",
      "   9 加える\r\n",
      "   9 致す\r\n",
      "   8 与える\r\n",
      "   7 告げる\r\n"
     ]
    }
   ],
   "source": [
    "!cut -f1 work/47.txt | sort | uniq -c | sort -r 2>/dev/null | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  30 する\tを 返事を\r\n",
      "  21 する\tを 挨拶を\r\n",
      "  16 する\tを 話を\r\n",
      "  14 する\tを 真似を\r\n",
      "  13 する\tを 喧嘩を\r\n",
      "   8 する\tを 質問を\r\n",
      "   7 する\tを 運動を\r\n",
      "   6 する\tを 注意を\r\n",
      "   6 する\tを 昼寝を\r\n",
      "   6 聞く\tを 話を\r\n"
     ]
    }
   ],
   "source": [
    "!cat work/47.txt | sort | uniq -c |  sort -r 2>/dev/null | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 48. 名詞から根へのパスの抽出<a id=\"sec9\"></a>\n",
    "文中のすべての名詞を含む文節に対し，その文節から構文木の根に至るパスを抽出せよ． ただし，構文木上のパスは以下の仕様を満たすものとする.  \n",
    "  \n",
    "* 各文節は（表層形の）形態素列で表現する  \n",
    "* パスの開始文節から終了文節に至るまで，各文節の表現を\"->\"で連結する  \n",
    "「吾輩はここで始めて人間というものを見た」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．  \n",
    "\n",
    "吾輩は -> 見た  \n",
    "ここで -> 始めて -> 人間という -> ものを -> 見た  \n",
    "人間という -> ものを -> 見た  \n",
    "ものを -> 見た  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing q48.py\n"
     ]
    }
   ],
   "source": [
    "%%file q48.py\n",
    "from q41 import parse\n",
    "from itertools import islice\n",
    "from cytoolz import nth\n",
    "\n",
    "def exist_noun(chunk):\n",
    "    for morph in chunk.morphs:\n",
    "        if morph.pos == \"名詞\":\n",
    "            return True\n",
    "        \n",
    "def chunk_is_root(chunk):\n",
    "    if chunk.dst == -1:\n",
    "        return True\n",
    "\n",
    "def noun_to_root(sentence):\n",
    "    roots = []\n",
    "    for chunk in sentence:\n",
    "        to_root = []\n",
    "        if exist_noun(chunk):\n",
    "            while(not(chunk_is_root(chunk))):\n",
    "                to_root.append(chunk)\n",
    "                chunk = sentence[chunk.dst]\n",
    "            to_root.append(chunk)\n",
    "            roots.append(to_root)\n",
    "            to_root=[]\n",
    "    return roots\n",
    "\n",
    "def make_pathes_list(line_index):\n",
    "    neko = parse()\n",
    "    pathes = [[chunk for chunk in chunk_list if len(chunk_list) > 1] for chunk_list in noun_to_root(nth(line_index, neko))]\n",
    "    return pathes\n",
    "\n",
    "def main():\n",
    "    for path in make_pathes_list(5):\n",
    "        print(\"->\".join([str(chunk) for chunk in path]))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "吾輩は->見た\r\n",
      "ここで->始めて->人間という->ものを->見た\r\n",
      "人間という->ものを->見た\r\n",
      "ものを->見た\r\n"
     ]
    }
   ],
   "source": [
    "!python q48.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 49. 名詞間の係り受けパスの抽出<a id=\"sec10\"></a>\n",
    "文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．ただし，名詞句ペアの文節番号がi\n",
    "とj\n",
    "（i<j\n",
    "）のとき，係り受けパスは以下の仕様を満たすものとする．\n",
    "\n",
    "* 問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を\"->\"で連結して表現する\n",
    "* 文節i\n",
    "とj\n",
    "に含まれる名詞句はそれぞれ，XとYに置換する  \n",
    "  \n",
    "  \n",
    "また，係り受けパスの形状は，以下の2通りが考えられる．\n",
    "\n",
    "* 文節i\n",
    "から構文木の根に至る経路上に文節j\n",
    "が存在する場合: 文節i\n",
    "から文節j\n",
    "のパスを表示\n",
    "* 上記以外で，文節i\n",
    "と文節j\n",
    "から構文木の根に至る経路上で共通の文節k\n",
    "で交わる場合: 文節i\n",
    "から文節k\n",
    "に至る直前のパスと文節j\n",
    "から文節k\n",
    "に至る直前までのパス，文節k\n",
    "の内容を\"|\"で連結して表示  \n",
    "  \n",
    "例えば，「吾輩はここで始めて人間というものを見た。」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．\n",
    "\n",
    "Xは | Yで -> 始めて -> 人間という -> ものを | 見た  \n",
    "Xは | Yという -> ものを | 見た  \n",
    "Xは | Yを | 見た  \n",
    "Xで -> 始めて -> Y  \n",
    "Xで -> 始めて -> 人間という -> Y  \n",
    "Xという -> Y  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting q49.py\n"
     ]
    }
   ],
   "source": [
    "%%file q49.py\n",
    "from q48 import make_pathes_list\n",
    "from itertools import combinations\n",
    "\n",
    "def rule(path1, path2):\n",
    "    for index_chunk1 in range(len(path1)):\n",
    "        for index_chunk2 in range(len(path2)):\n",
    "            if path1[index_chunk1] == path2[index_chunk2]:\n",
    "                k = path1[index_chunk1] #文節k\n",
    "                i_to_k = [str(path1[i_ch1]) if i_ch1 > 0 else path1[i_ch1].surface_q49(\"X\")\n",
    "                          for i_ch1 in range(index_chunk1)]\n",
    "                j_to_k = [str(path2[i_ch2]) if i_ch2 > 0 else path2[i_ch2].surface_q49(\"Y\")\n",
    "                          for i_ch2 in range(index_chunk2)]\n",
    "                return [i_to_k, j_to_k, k]\n",
    "def main():\n",
    "    pathes = make_pathes_list(5)\n",
    "    pathes_q49 = [] #抽出されたパスのリスト\n",
    "    for path_comb in combinations(pathes, 2):\n",
    "        pathes_q49.append(rule(*path_comb))\n",
    "    for path_q49 in pathes_q49:\n",
    "        if isinstance(path_q49, type(None)): pass\n",
    "        elif not(len(path_q49[0]) and len(path_q49[1])) :\n",
    "            print(\" -> \".join(path_q49[0]) + \" -> \".join(path_q49[1]) + \" -> \" + path_q49[2].surface_q49(\"Y\"))\n",
    "        else:\n",
    "            print(\"->\".join(path_q49[0]) + \" | \" + \" -> \".join(path_q49[1]) + \" | \" + str(path_q49[2]))\n",
    "\n",
    "if __name__ ==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xは | Yで -> 始めて -> 人間という -> ものを | 見た\r\n",
      "Xは | Yという -> ものを | 見た\r\n",
      "Xは | Yを | 見た\r\n",
      "Xで -> 始めて -> Yという\r\n",
      "Xで -> 始めて -> 人間という -> Yを\r\n",
      "Xという -> Yを\r\n"
     ]
    }
   ],
   "source": [
    "!python q49.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
